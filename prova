import requests

def query_huggingface_inference_api(prompt, model_id, hf_token, max_new_tokens=200):
    """
    Invia un prompt a un modello Hugging Face tramite l'Inference API e restituisce il testo generato.

    Args:
        prompt (str): Il testo da inviare come input.
        model_id (str): Il modello da usare (es. "HuggingFaceH4/zephyr-7b-beta").
        hf_token (str): Il token Hugging Face personale con permesso read.
        max_new_tokens (int): Numero massimo di nuovi token da generare.

    Returns:
        str: Il testo generato dal modello.
    """
    api_url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {hf_token}"
    }

    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": max_new_tokens,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True,
            "return_full_text": False
        }
    }

    response = requests.post(api_url, headers=headers, json=payload)

    if response.status_code == 200:
        result = response.json()
        return result[0]["generated_text"].strip()
    else:
        raise Exception(f"API Error {response.status_code}: {response.text}")

if __name__ == "__main__":
    HF_TOKEN = "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"  # usa il tuo token personale
    MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"           # modello gratuito e attivo

    prompt = """
You are a medical assistant. Based on the patient's history, generate a brief summary.

Patient: 45-year-old male, diagnosed with type 1 diabetes and hypertension.
Current medication: insulin and metformin.
"""

    print("ðŸ”Ž Sending prompt to Hugging Face...")
    result = query_huggingface_inference_api(prompt, MODEL_ID, HF_TOKEN)
    print("\nðŸ“‹ Generated Output:\n")
    print(result)
