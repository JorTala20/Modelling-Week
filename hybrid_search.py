from __future__ import annotations
"""
data_pipeline_hybrid.py Â· VersiÃ³n local Â· sin Elasticsearch
-----------------------------------------------------------
â€¢ Ingesta unificada:  Synthea (+patients/+conditions),
                      PubMed-RCT (Kaggle TSV),
                      ClinicalTrials.gov (API v2).
â€¢ VectorizaciÃ³n con sentence-transformers/MiniLM-L6-v2.
â€¢ Ãndice vectorial FAISS  (o HNSW si FAISS no estÃ¡ disponible).
â€¢ BM25 local (rank_bm25) + Vector search  +  RRF.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import argparse, csv, json, os, sys
from pathlib import Path
from typing import List, Dict

import numpy as np, pandas as pd
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from urllib.parse import quote
import requests

try:
    import faiss;  _HAS_FAISS = True
except ImportError:
    _HAS_FAISS = False
try:
    import hnswlib; _HAS_HNSW = True
except ImportError:
    _HAS_HNSW = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ParÃ¡metros / ficheros â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOC_IDS_NPY, VEC_FAISS, VEC_HNSW = "doc_ids.npy", "vectors.index", "hnsw_index.bin"
DIM, MODEL_NAME = 384, "sentence-transformers/all-MiniLM-L6-v2"
CORPUS_CSV = "corpus.csv"

import spacy
import scispacy
import scispacy.umls_linking
from urllib.parse import quote
import requests
import warnings
from sklearn.exceptions import InconsistentVersionWarning
import random

# Ignorar advertencias de versiÃ³n de sklearn que a veces surgen con scispacy
warnings.filterwarnings("ignore", category=InconsistentVersionWarning)

# --- Funciones proporcionadas por el usuario (sin cambios) ---

def crear_pipeline_umls():
    """Carga el modelo de spacy y aÃ±ade el linker de UMLS."""
    print("Cargando modelo de spaCy y linker UMLS...")
    nlp = spacy.load("en_core_sci_sm")
    nlp.add_pipe(
        "scispacy_linker",
        last=True,
        config={
            "linker_name": "umls",
            "resolve_abbreviations": True
        },
    )
    linker = nlp.get_pipe("scispacy_linker")
    print("Â¡Modelo cargado con Ã©xito!")
    return nlp, linker

def extraer_cui(texto: str, nlp, linker) -> dict:
    """Extrae entidades mÃ©dicas, sus CUIs y sinÃ³nimos de un texto."""
    doc = nlp(texto)
    resultados = []
    for ent in doc.ents:
        kb_ents = getattr(ent._, "kb_ents", None)
        if not kb_ents:
            continue
        cui = kb_ents[0][0]
        entity = linker.kb.cui_to_entity.get(cui)
        if not entity:
            continue
        # Usamos un set para sinÃ³nimos Ãºnicos y lo convertimos a lista
        resultados.append(cui)
    return resultados

# --- 1. FunciÃ³n de NormalizaciÃ³n del Prompt (Nueva) ---

def normalizar_prompt_a_cui(texto_paciente: str, nlp) -> str:
    """
    Toma un texto, detecta entidades mÃ©dicas y las reemplaza por su CUI.

    Args:
        texto_paciente: El perfil del paciente en lenguaje natural.
        nlp: El pipeline de spaCy cargado con el linker UMLS.

    Returns:
        Un string donde las entidades mÃ©dicas han sido sustituidas por sus CUIs.
    """
    print(f"\n--- Normalizando a CUI ---")
    print(f"Texto original: '{texto_paciente}'")
    
    doc = nlp(texto_paciente)
    texto_modificado = texto_paciente
    
    # Obtenemos las entidades y las ordenamos por su posiciÃ³n inicial en orden inverso
    # Esto es CRÃTICO para que los reemplazos no invaliden los Ã­ndices de las siguientes entidades
    entidades_orden_inverso = sorted(doc.ents, key=lambda e: e.start_char, reverse=True)
    
    entidades_encontradas = 0
    for ent in entidades_orden_inverso:
        # Comprobamos si la entidad tiene candidatos en la base de conocimiento (UMLS)
        if hasattr(ent._, "kb_ents") and ent._.kb_ents:
            # Seleccionamos el CUI del primer candidato (el de mayor probabilidad)
            cui = ent._.kb_ents[0][0]
            
            # Reemplazamos el texto de la entidad por su CUI en la cadena
            inicio = ent.start_char
            fin = ent.end_char
            texto_modificado = texto_modificado[:inicio] + cui + texto_modificado[fin:]
            entidades_encontradas += 1

    if entidades_encontradas == 0:
        print("âš ï¸ No se encontraron entidades mÃ©dicas para reemplazar.")
    else:
        print(f"âœ… Se reemplazaron {entidades_encontradas} entidad(es).")

    return texto_modificado

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ingesta Synthea CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ingest_synthea_csv(dir_csv: str):
    
    def safe(val, slice_len=None):
        if pd.notna(val):
            val_str = str(val)
            return val_str[:slice_len] if slice_len else val_str
        return "unknown"

    patients = pd.read_csv(Path(dir_csv) / "patients.csv", dtype=str)
    cond = pd.read_csv(Path(dir_csv) / "conditions.csv", dtype=str)
    allergies = pd.read_csv(Path(dir_csv) / "allergies.csv", dtype=str)
    care = pd.read_csv(Path(dir_csv) / "careplans.csv", dtype=str)
    dev = pd.read_csv(Path(dir_csv) / "devices.csv", dtype=str)
    encounters = pd.read_csv(Path(dir_csv) / "encounters.csv", dtype=str)
    imaging = pd.read_csv(Path(dir_csv) / "imaging_studies.csv", dtype=str)
    immun = pd.read_csv(Path(dir_csv) / "immunizations.csv", dtype=str)
    meds = pd.read_csv(Path(dir_csv) / "medications.csv", dtype=str)
    obs = pd.read_csv(Path(dir_csv) / "observations.csv", dtype=str)
    proc = pd.read_csv(Path(dir_csv) / "procedures.csv", dtype=str)

    textos, ids = [], []
    for _, p in patients.iterrows():
        pid = p["Id"]
        textos.append(f"Patient {safe(p['GENDER'])} born {safe(p['BIRTHDATE'], 10)} race {safe(p['RACE'])}")
        ids.append(f"ehr_{pid}_demo")

        for idx, c in cond[cond["PATIENT"] == pid].iterrows():
            textos.append(f"Condition {safe(c['DESCRIPTION'])} started {safe(c['START'], 10)}")
            ids.append(f"ehr_{pid}_cond_{idx}")

        for idx, e in encounters[encounters["PATIENT"] == pid].iterrows():
            textos.append(f"Encounter {safe(e['ENCOUNTERCLASS'])} for {safe(e['DESCRIPTION'])} from {safe(e['START'], 10)} to {safe(e['STOP'], 10)}")
            ids.append(f"ehr_{pid}_enc_{idx}")

        for idx, a in allergies[allergies["PATIENT"] == pid].iterrows():
            textos.append(f"Allergy to {safe(a['DESCRIPTION'])} recorded on {safe(a['START'], 10)}")
            ids.append(f"ehr_{pid}_alg_{idx}")

        for idx, c in care[care["PATIENT"] == pid].iterrows():
            textos.append(f"Care plan: {safe(c['DESCRIPTION'])} from {safe(c['START'], 10)} to {safe(c['STOP'], 10)}")
            ids.append(f"ehr_{pid}_care_{idx}")

        for idx, d in dev[dev["PATIENT"] == pid].iterrows():
            textos.append(f"Device: {safe(d['DESCRIPTION'])} implanted on {safe(d['START'], 10)}")
            ids.append(f"ehr_{pid}_dev_{idx}")

        for idx, im in imaging[imaging["PATIENT"] == pid].iterrows():
            textos.append(f"Imaging study: {safe(im['BODYSITE_DESCRIPTION'])} on {safe(im['DATE'], 10)}")
            ids.append(f"ehr_{pid}_img_{idx}")

        for idx, im in immun[immun["PATIENT"] == pid].iterrows():
            textos.append(f"Immunization: {safe(im['DESCRIPTION'])} on {safe(im['DATE'], 10)}")
            ids.append(f"ehr_{pid}_imm_{idx}")

        for idx, m in meds[meds["PATIENT"] == pid].iterrows():
            textos.append(f"Medication: {safe(m['DESCRIPTION'])} from {safe(m['START'], 10)} to {safe(m['STOP'], 10)}")
            ids.append(f"ehr_{pid}_med_{idx}")

        for idx, o in obs[obs["PATIENT"] == pid].iterrows():
            textos.append(f"Observation: {safe(o['DESCRIPTION'])} = {safe(o['VALUE'])} on {safe(o['DATE'], 10)}")
            ids.append(f"ehr_{pid}_obs_{idx}")

        for idx, pr in proc[proc["PATIENT"] == pid].iterrows():
            textos.append(f"Procedure: {safe(pr['DESCRIPTION'])} on {safe(pr['DATE'], 10)}")
            ids.append(f"ehr_{pid}_proc_{idx}")

    print(f"âœ”ï¸  Synthea fragmentos: {len(textos)}")
    return textos, ids


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ingesta PubMed-RCT TSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ingest_pubmed_csv(csv_path: str):
    df = pd.read_csv(csv_path, dtype=str)

    # Verifica que existen las columnas requeridas
    if "abstract_text" not in df.columns or "target" not in df.columns:
        raise ValueError("El CSV debe contener las columnas 'abstract_text' y 'target'.")

    # Elimina filas con texto o target vacÃ­o
    df = df[df["abstract_text"].notna() & df["target"].notna()]

    # Asegura que todo es texto plano
    df["abstract_text"] = df["abstract_text"].astype(str)
    df["target"] = df["target"].astype(str)

    # Concatena target al texto â†’ formato: "TARGET: texto..."
    textos = [f"{row['target']}: {row['abstract_text']}" for _, row in df.iterrows()]
    ids    = [f"pm_{i}" for i in range(len(textos))]

    print(f"âœ”ï¸  PubMed-RCT fragmentos: {len(textos)}")
    return textos, ids


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ingesta ClinicalTrials.gov â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ingest_clinical_trials(term: str, limit: int = 100):
    url = f"https://clinicaltrials.gov/api/v2/studies?query.term={quote(term)}&pageSize={limit}"
    data = requests.get(url, timeout=30).json()
    estudios = data.get("studies", [])
    textos, ids = [], []
    for i, e in enumerate(estudios):
        ps = e.get("protocolSection", {})
        title   = ps.get("identificationModule", {}).get("briefTitle", "")
        summary = ps.get("descriptionModule", {}).get("briefSummary", "")
        full = f"{title}. {summary}".strip()
        if full:
            textos.append(full)
            ids.append(f"ct_{i}")
    print(f"âœ”ï¸  ClinicalTrials fragmentos: {len(textos)}")
    return textos, ids

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ingesta unificada â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ingest_all(synthea_path: str,
               pubmed_path: str,
               trial_term: str,
               trial_limit: int = 500):
    textos, ids = [], []
    textos += (ts := ingest_synthea_csv(synthea_path))[0];   ids += ts[1]
    textos += (tp := ingest_pubmed_csv(pubmed_path))[0];     ids += tp[1]
    textos += (tc := ingest_clinical_trials(trial_term, trial_limit))[0]; ids += tc[1]
    return textos, ids

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VectorizaciÃ³n / Ãndice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_vector_index(textos: List[str], ids: List[str]):
    model = SentenceTransformer(MODEL_NAME)
    emb   = model.encode(textos, normalize_embeddings=True)
    np.save(DOC_IDS_NPY, np.array(ids))

    if _HAS_FAISS:
        idx = faiss.IndexFlatIP(DIM); idx.add(emb.astype(np.float32))
        faiss.write_index(idx, VEC_FAISS); print("âœ… FAISS guardado")
    elif _HAS_HNSW:
        p = hnswlib.Index(space="cosine", dim=DIM)
        p.init_index(max_elements=len(emb), ef_construction=200, M=16)
        p.add_items(emb, np.arange(len(emb))); p.save_index(VEC_HNSW)
        print("âœ… HNSW guardado")
    else:
        raise ImportError("Instala faiss-cpu o hnswlib")

    pd.DataFrame({"id": ids, "text": textos}).to_csv(CORPUS_CSV, index=False)
    print("ğŸ’¾ corpus.csv generado")
    return model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BM25 local â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bm25_scores(textos: List[str], query: str, nlp):
    print(textos)
    cleaned = [str(t).lower().split() for t in textos]          # â† cast a str
    print(cleaned)
    bm25 = BM25Okapi(normalizar_prompt_a_cui(cleaned, nlp))
    return bm25.get_scores(query.lower().split())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Vector search local â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vec_scores(query: str, model: SentenceTransformer, top_n=200):
    q = model.encode([query], normalize_embeddings=True).astype(np.float32)
    ids = np.load(DOC_IDS_NPY)
    if Path(VEC_FAISS).exists() and _HAS_FAISS:
        idx = faiss.read_index(VEC_FAISS)
        D, I = idx.search(q, top_n); sims = 1 - D[0]
        return {str(ids[i]): float(sims[j]) for j, i in enumerate(I[0])}
    idx = hnswlib.Index(space="cosine", dim=DIM); idx.load_index(VEC_HNSW)
    lab, dist = idx.knn_query(q, k=top_n); sims = 1 - dist[0]
    return {str(ids[i]): float(sims[j]) for j, i in enumerate(lab[0])}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RRF fusion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rrf_fuse(bm25, vec, doc_ids, k=20, rrf_k=5):
    scr = {}
    # BM25 ranks
    for rk, idx in enumerate(np.argsort(bm25)[::-1], 1):
        scr[doc_ids[idx]] = scr.get(doc_ids[idx], 0) + 1/(rrf_k+rk)
    # Vector ranks
    for rk, doc in enumerate(sorted(vec, key=vec.get, reverse=True), 1):
        scr[doc] = scr.get(doc, 0) + 1/(rrf_k+rk)
    return sorted(scr.items(), key=lambda x: x[1], reverse=True)[:k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cli(nlp):
    argp = argparse.ArgumentParser()
    sub  = argp.add_subparsers(dest="cmd", required=True)

    p_ing = sub.add_parser("ingest")
    p_ing.add_argument("--synthea", default="synthea/csv")
    p_ing.add_argument("--pubmed",  default="pubmed_rct/train.csv")
    p_ing.add_argument("--trial",   default="cancer")
    p_ing.add_argument("--trial_limit", type=int, default=100)

    p_s = sub.add_parser("search")
    p_s.add_argument("prompt")
    p_s.add_argument("--k", type=int, default=20)

    args = argp.parse_args()

    if args.cmd == "ingest":
        txt, ids = ingest_all(args.synthea, args.pubmed, args.trial, args.trial_limit)
        build_vector_index(txt, ids)
        print("ğŸš€ Ingesta y vectorizaciÃ³n completas.")

    elif args.cmd == "search":
        if not Path(CORPUS_CSV).exists():
            print("ğŸ”„ No corpus.csv. Ejecutando ingesta rÃ¡pida...")
            txt, ids = ingest_all("synthea/csv", "pubmed_rct/train.csv", "35-year-old male with lung cancer and ibuprofen 3 times per week", 200)
            build_vector_index(txt, ids)
        df = pd.read_csv(CORPUS_CSV).fillna("")        # â† elimina NaN
        text = df["text"].astype(str).tolist()        # â† forzamos str

        doc_ids = df["id"].tolist()
        model = SentenceTransformer(MODEL_NAME)
        bm    = bm25_scores(text, args.prompt, nlp)
        vec   = vec_scores(args.prompt, model)
        res   = rrf_fuse(bm, vec, doc_ids, k=args.k)

        print("\nğŸ“„ Top resultados:")
        for did, sc in res:
            snippet = df[df.id==did].text.values[0][:120].replace("\n", " ")
            print(f"{did:15}  score={sc:.3f}  â†’ {snippet}...")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Auto-ejecuciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ != "__main__":
    if len(sys.argv) == 1:
        sys.argv.extend(["search", "35-year-old male with lung cancer and ibuprofen 3 times per week"])
    cli(nlp)
